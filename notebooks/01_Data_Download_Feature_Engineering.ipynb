{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Forex Data Download & Feature Engineering\n",
        "## Machine Learning for Forex Trading Research\n",
        "\n",
        "**Notebook n√†y bao g·ªìm:**\n",
        "1. T·∫£i d·ªØ li·ªáu Forex t·ª´ yfinance (2020-2025)\n",
        "2. Feature Engineering v·ªõi 110+ features\n",
        "3. L∆∞u dataset s·∫µn s√†ng cho ML\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. C√†i ƒê·∫∑t Th∆∞ Vi·ªán"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libs"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance pandas numpy -q\n",
        "print(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t th∆∞ vi·ªán!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Th∆∞ Vi·ªán"
      ],
      "metadata": {
        "id": "imports"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Import th√†nh c√¥ng!\")"
      ],
      "metadata": {
        "id": "imports_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. C·∫•u H√¨nh"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# C·∫§U H√åNH\n",
        "# ============================================\n",
        "\n",
        "# 5 c·∫∑p ti·ªÅn Major ƒë·∫°i di·ªán\n",
        "CURRENCY_PAIRS = {\n",
        "    'EURUSD=X': 'EUR_USD',   # Euro vs US Dollar\n",
        "    'GBPUSD=X': 'GBP_USD',   # British Pound vs US Dollar\n",
        "    'USDJPY=X': 'USD_JPY',   # US Dollar vs Japanese Yen\n",
        "    'AUDUSD=X': 'AUD_USD',   # Australian Dollar vs US Dollar\n",
        "    'USDCHF=X': 'USD_CHF'    # US Dollar vs Swiss Franc\n",
        "}\n",
        "\n",
        "# Th·ªùi gian nghi√™n c·ª©u\n",
        "START_DATE = '2020-01-01'\n",
        "END_DATE = '2025-12-31'\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c\n",
        "os.makedirs('data/raw/daily', exist_ok=True)\n",
        "os.makedirs('data/raw/4h', exist_ok=True)\n",
        "os.makedirs('data/raw/1h', exist_ok=True)\n",
        "os.makedirs('data/processed/daily', exist_ok=True)\n",
        "os.makedirs('data/processed/4h', exist_ok=True)\n",
        "os.makedirs('data/processed/1h', exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ C·∫•u h√¨nh ho√†n t·∫•t!\")\n",
        "print(f\"üìÖ Th·ªùi gian: {START_DATE} ƒë·∫øn {END_DATE}\")\n",
        "print(f\"üí± S·ªë c·∫∑p ti·ªÅn: {len(CURRENCY_PAIRS)}\")"
      ],
      "metadata": {
        "id": "config_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# PH·∫¶N 1: T·∫¢I D·ªÆ LI·ªÜU\n",
        "---"
      ],
      "metadata": {
        "id": "part1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. H√†m T·∫£i D·ªØ Li·ªáu Daily"
      ],
      "metadata": {
        "id": "daily_func"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_daily_data(ticker, name):\n",
        "    \"\"\"T·∫£i d·ªØ li·ªáu khung Daily t·ª´ 2020\"\"\"\n",
        "    print(f\"  üì• ƒêang t·∫£i {name} Daily...\")\n",
        "    \n",
        "    try:\n",
        "        data = yf.download(\n",
        "            ticker, \n",
        "            start=START_DATE, \n",
        "            end=END_DATE, \n",
        "            interval='1d',\n",
        "            progress=False\n",
        "        )\n",
        "        \n",
        "        if len(data) == 0:\n",
        "            print(f\"  ‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu cho {name}\")\n",
        "            return None\n",
        "        \n",
        "        # L√†m s·∫°ch d·ªØ li·ªáu\n",
        "        data = data.reset_index()\n",
        "        \n",
        "        # ƒê·ªïi t√™n c·ªôt n·∫øu l√† MultiIndex\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            data.columns = [col[0] if col[1] == '' else col[0] for col in data.columns]\n",
        "        \n",
        "        # Chu·∫©n h√≥a t√™n c·ªôt\n",
        "        data.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "        data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "        \n",
        "        # L∆∞u file\n",
        "        output_path = f'data/raw/daily/{name}_daily.csv'\n",
        "        data.to_csv(output_path, index=False)\n",
        "        print(f\"  ‚úÖ ƒê√£ l∆∞u {name} Daily: {len(data)} records\")\n",
        "        \n",
        "        return data\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå L·ªói khi t·∫£i {name}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "daily_func_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. H√†m T·∫£i D·ªØ Li·ªáu 1H (Chunked)"
      ],
      "metadata": {
        "id": "hourly_func"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_hourly_data(ticker, name):\n",
        "    \"\"\"\n",
        "    T·∫£i d·ªØ li·ªáu 1H\n",
        "    L∆∞u √Ω: yfinance gi·ªõi h·∫°n d·ªØ li·ªáu 1h trong 730 ng√†y g·∫ßn nh·∫•t\n",
        "    \"\"\"\n",
        "    print(f\"  üì• ƒêang t·∫£i {name} 1H...\")\n",
        "    \n",
        "    try:\n",
        "        # yfinance ch·ªâ cho ph√©p t·∫£i 730 ng√†y v·ªõi interval 1h\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=729)\n",
        "        \n",
        "        data = yf.download(\n",
        "            ticker,\n",
        "            start=start_date.strftime('%Y-%m-%d'),\n",
        "            end=end_date.strftime('%Y-%m-%d'),\n",
        "            interval='1h',\n",
        "            progress=False\n",
        "        )\n",
        "        \n",
        "        if len(data) == 0:\n",
        "            print(f\"  ‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu 1H cho {name}\")\n",
        "            return None\n",
        "        \n",
        "        # L√†m s·∫°ch d·ªØ li·ªáu\n",
        "        data = data.reset_index()\n",
        "        \n",
        "        # ƒê·ªïi t√™n c·ªôt n·∫øu l√† MultiIndex\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            data.columns = [col[0] if col[1] == '' else col[0] for col in data.columns]\n",
        "        \n",
        "        # Chu·∫©n h√≥a t√™n c·ªôt\n",
        "        if 'Datetime' in data.columns:\n",
        "            data.rename(columns={'Datetime': 'Date'}, inplace=True)\n",
        "        \n",
        "        data.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "        data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "        \n",
        "        # L∆∞u file 1H\n",
        "        output_path = f'data/raw/1h/{name}_1h.csv'\n",
        "        data.to_csv(output_path, index=False)\n",
        "        print(f\"  ‚úÖ ƒê√£ l∆∞u {name} 1H: {len(data)} records\")\n",
        "        \n",
        "        return data\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå L·ªói khi t·∫£i {name} 1H: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "hourly_func_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. H√†m Resample sang 4H"
      ],
      "metadata": {
        "id": "resample_func"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_to_4h(data_1h, name):\n",
        "    \"\"\"Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu 1H th√†nh 4H\"\"\"\n",
        "    if data_1h is None or len(data_1h) == 0:\n",
        "        return None\n",
        "    \n",
        "    print(f\"  üîÑ ƒêang chuy·ªÉn ƒë·ªïi {name} sang 4H...\")\n",
        "    \n",
        "    try:\n",
        "        df = data_1h.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df.set_index('Date', inplace=True)\n",
        "        \n",
        "        # Resample sang 4H\n",
        "        data_4h = df.resample('4h').agg({\n",
        "            'Open': 'first',\n",
        "            'High': 'max',\n",
        "            'Low': 'min',\n",
        "            'Close': 'last',\n",
        "            'Volume': 'sum'\n",
        "        }).dropna()\n",
        "        \n",
        "        data_4h = data_4h.reset_index()\n",
        "        \n",
        "        # L∆∞u file 4H\n",
        "        output_path = f'data/raw/4h/{name}_4h.csv'\n",
        "        data_4h.to_csv(output_path, index=False)\n",
        "        print(f\"  ‚úÖ ƒê√£ l∆∞u {name} 4H: {len(data_4h)} records\")\n",
        "        \n",
        "        return data_4h\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå L·ªói khi chuy·ªÉn ƒë·ªïi {name} sang 4H: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "resample_func_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. üöÄ T·∫£i To√†n B·ªô D·ªØ Li·ªáu"
      ],
      "metadata": {
        "id": "download_all"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"üöÄ B·∫ÆT ƒê·∫¶U T·∫¢I D·ªÆ LI·ªÜU FOREX\")\n",
        "print(f\"üìÖ Th·ªùi gian: {START_DATE} ƒë·∫øn {END_DATE}\")\n",
        "print(f\"üí± S·ªë c·∫∑p ti·ªÅn: {len(CURRENCY_PAIRS)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# L∆∞u tr·ªØ d·ªØ li·ªáu\n",
        "all_data = {}\n",
        "\n",
        "# T·∫£i d·ªØ li·ªáu cho t·ª´ng c·∫∑p ti·ªÅn\n",
        "for ticker, name in CURRENCY_PAIRS.items():\n",
        "    print(f\"\\nüìä ƒêang x·ª≠ l√Ω {name}...\")\n",
        "    \n",
        "    all_data[name] = {}\n",
        "    \n",
        "    # T·∫£i Daily\n",
        "    all_data[name]['daily'] = download_daily_data(ticker, name)\n",
        "    \n",
        "    # T·∫£i 1H\n",
        "    all_data[name]['1h'] = download_hourly_data(ticker, name)\n",
        "    \n",
        "    # T·∫°o 4H t·ª´ 1H\n",
        "    if all_data[name]['1h'] is not None:\n",
        "        all_data[name]['4h'] = resample_to_4h(all_data[name]['1h'], name)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ HO√ÄN TH√ÄNH T·∫¢I D·ªÆ LI·ªÜU!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "download_all_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# PH·∫¶N 2: FEATURE ENGINEERING\n",
        "---"
      ],
      "metadata": {
        "id": "part2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Tham S·ªë Feature Engineering"
      ],
      "metadata": {
        "id": "fe_params"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tham s·ªë cho indicators\n",
        "RSI_PERIOD = 14\n",
        "MACD_FAST = 12\n",
        "MACD_SLOW = 26\n",
        "MACD_SIGNAL = 9\n",
        "BB_PERIOD = 20\n",
        "BB_STD = 2\n",
        "ATR_PERIOD = 14\n",
        "STOCH_PERIOD = 14\n",
        "LAG_PERIODS = [1, 2, 3, 5, 7, 14, 21]\n",
        "MA_PERIODS = [7, 14, 21, 50, 100, 200]\n",
        "\n",
        "print(\"‚úÖ Tham s·ªë ƒë√£ c·∫•u h√¨nh!\")"
      ],
      "metadata": {
        "id": "fe_params_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. H√†m T√≠nh Technical Indicators"
      ],
      "metadata": {
        "id": "tech_ind"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rsi(close, period=14):\n",
        "    \"\"\"T√≠nh RSI\"\"\"\n",
        "    delta = close.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_macd(close, fast=12, slow=26, signal=9):\n",
        "    \"\"\"T√≠nh MACD\"\"\"\n",
        "    ema_fast = close.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = close.ewm(span=slow, adjust=False).mean()\n",
        "    macd_line = ema_fast - ema_slow\n",
        "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
        "    histogram = macd_line - signal_line\n",
        "    return macd_line, signal_line, histogram\n",
        "\n",
        "def calculate_bollinger_bands(close, period=20, std_dev=2):\n",
        "    \"\"\"T√≠nh Bollinger Bands\"\"\"\n",
        "    sma = close.rolling(window=period).mean()\n",
        "    std = close.rolling(window=period).std()\n",
        "    upper = sma + (std * std_dev)\n",
        "    lower = sma - (std * std_dev)\n",
        "    width = (upper - lower) / sma\n",
        "    position = (close - lower) / (upper - lower)\n",
        "    return upper, sma, lower, width, position\n",
        "\n",
        "def calculate_atr(high, low, close, period=14):\n",
        "    \"\"\"T√≠nh ATR\"\"\"\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift(1))\n",
        "    tr3 = abs(low - close.shift(1))\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=period).mean()\n",
        "\n",
        "def calculate_stochastic(high, low, close, period=14):\n",
        "    \"\"\"T√≠nh Stochastic\"\"\"\n",
        "    lowest = low.rolling(window=period).min()\n",
        "    highest = high.rolling(window=period).max()\n",
        "    k = 100 * (close - lowest) / (highest - lowest)\n",
        "    d = k.rolling(window=3).mean()\n",
        "    return k, d\n",
        "\n",
        "print(\"‚úÖ C√°c h√†m Technical Indicators ƒë√£ s·∫µn s√†ng!\")"
      ],
      "metadata": {
        "id": "tech_ind_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. H√†m Feature Engineering Ch√≠nh"
      ],
      "metadata": {
        "id": "fe_main"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_all_features(df):\n",
        "    \"\"\"\n",
        "    Th√™m t·∫•t c·∫£ features v√†o DataFrame\n",
        "    Input: DataFrame v·ªõi c·ªôt Date, Open, High, Low, Close, Volume\n",
        "    Output: DataFrame v·ªõi ~110+ features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # ========================\n",
        "    # PRICE FEATURES\n",
        "    # ========================\n",
        "    df['Return'] = df['Close'].pct_change()\n",
        "    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "    df['Price_Change'] = df['Close'] - df['Open']\n",
        "    df['Price_Change_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
        "    df['Range'] = df['High'] - df['Low']\n",
        "    df['Body'] = abs(df['Close'] - df['Open'])\n",
        "    df['Upper_Shadow'] = df['High'] - df[['Open', 'Close']].max(axis=1)\n",
        "    df['Lower_Shadow'] = df[['Open', 'Close']].min(axis=1) - df['Low']\n",
        "    df['Candle_Color'] = np.where(df['Close'] > df['Open'], 1, 0)\n",
        "    df['Gap'] = df['Open'] - df['Close'].shift(1)\n",
        "    \n",
        "    # Volatility\n",
        "    for p in [5, 10, 20]:\n",
        "        df[f'Volatility_{p}'] = df['Return'].rolling(window=p).std()\n",
        "    \n",
        "    # ========================\n",
        "    # TECHNICAL INDICATORS\n",
        "    # ========================\n",
        "    # RSI\n",
        "    df['RSI'] = calculate_rsi(df['Close'], RSI_PERIOD)\n",
        "    df['RSI_overbought'] = (df['RSI'] > 70).astype(int)\n",
        "    df['RSI_oversold'] = (df['RSI'] < 30).astype(int)\n",
        "    \n",
        "    # MACD\n",
        "    df['MACD'], df['MACD_signal'], df['MACD_histogram'] = calculate_macd(\n",
        "        df['Close'], MACD_FAST, MACD_SLOW, MACD_SIGNAL\n",
        "    )\n",
        "    df['MACD_crossover'] = np.where(df['MACD'] > df['MACD_signal'], 1, -1)\n",
        "    \n",
        "    # Bollinger Bands\n",
        "    df['BB_upper'], df['BB_middle'], df['BB_lower'], df['BB_width'], df['BB_position'] = \\\n",
        "        calculate_bollinger_bands(df['Close'], BB_PERIOD, BB_STD)\n",
        "    \n",
        "    # ATR\n",
        "    df['ATR'] = calculate_atr(df['High'], df['Low'], df['Close'], ATR_PERIOD)\n",
        "    df['ATR_percent'] = df['ATR'] / df['Close'] * 100\n",
        "    \n",
        "    # Stochastic\n",
        "    df['Stoch_K'], df['Stoch_D'] = calculate_stochastic(\n",
        "        df['High'], df['Low'], df['Close'], STOCH_PERIOD\n",
        "    )\n",
        "    \n",
        "    # Momentum & ROC\n",
        "    df['Momentum_10'] = df['Close'] - df['Close'].shift(10)\n",
        "    df['ROC_10'] = ((df['Close'] - df['Close'].shift(10)) / df['Close'].shift(10)) * 100\n",
        "    \n",
        "    # Moving Averages\n",
        "    for p in MA_PERIODS:\n",
        "        if len(df) > p:\n",
        "            df[f'SMA_{p}'] = df['Close'].rolling(window=p).mean()\n",
        "            df[f'EMA_{p}'] = df['Close'].ewm(span=p, adjust=False).mean()\n",
        "            df[f'Close_vs_SMA_{p}'] = (df['Close'] / df[f'SMA_{p}'] - 1) * 100\n",
        "    \n",
        "    # MA Crossovers\n",
        "    if 'SMA_7' in df.columns and 'SMA_21' in df.columns:\n",
        "        df['MA_7_21_cross'] = np.where(df['SMA_7'] > df['SMA_21'], 1, -1)\n",
        "    if 'SMA_50' in df.columns and 'SMA_200' in df.columns:\n",
        "        df['MA_50_200_cross'] = np.where(df['SMA_50'] > df['SMA_200'], 1, -1)\n",
        "    \n",
        "    # ========================\n",
        "    # LAGGED FEATURES\n",
        "    # ========================\n",
        "    for lag in LAG_PERIODS:\n",
        "        if len(df) > lag:\n",
        "            df[f'Close_lag_{lag}'] = df['Close'].shift(lag)\n",
        "            df[f'Return_lag_{lag}'] = df['Return'].shift(lag)\n",
        "    \n",
        "    # RSI lags\n",
        "    for lag in [1, 3, 5]:\n",
        "        df[f'RSI_lag_{lag}'] = df['RSI'].shift(lag)\n",
        "    \n",
        "    # Cumulative Returns\n",
        "    for p in [3, 5, 10, 20]:\n",
        "        if len(df) > p:\n",
        "            df[f'Cumulative_Return_{p}'] = df['Close'].pct_change(periods=p)\n",
        "    \n",
        "    # Rolling Stats\n",
        "    for p in [5, 10, 20]:\n",
        "        if len(df) > p:\n",
        "            df[f'Rolling_Mean_{p}'] = df['Close'].rolling(window=p).mean()\n",
        "            df[f'Rolling_Std_{p}'] = df['Close'].rolling(window=p).std()\n",
        "            df[f'Rolling_Min_{p}'] = df['Close'].rolling(window=p).min()\n",
        "            df[f'Rolling_Max_{p}'] = df['Close'].rolling(window=p).max()\n",
        "    \n",
        "    # ========================\n",
        "    # TIME FEATURES\n",
        "    # ========================\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
        "    df['Day_of_Month'] = df['Date'].dt.day\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Quarter'] = df['Date'].dt.quarter\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Week_of_Year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "    \n",
        "    # Binary time features\n",
        "    df['Is_Monday'] = (df['Day_of_Week'] == 0).astype(int)\n",
        "    df['Is_Friday'] = (df['Day_of_Week'] == 4).astype(int)\n",
        "    df['Is_Month_Start'] = df['Date'].dt.is_month_start.astype(int)\n",
        "    df['Is_Month_End'] = df['Date'].dt.is_month_end.astype(int)\n",
        "    \n",
        "    # Cyclical encoding\n",
        "    df['Day_sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)\n",
        "    df['Day_cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)\n",
        "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "    \n",
        "    # ========================\n",
        "    # TARGET VARIABLES\n",
        "    # ========================\n",
        "    # Classification: Direction\n",
        "    df['Target_Direction'] = np.where(\n",
        "        df['Close'].shift(-1) > df['Close'], 1, 0\n",
        "    )\n",
        "    \n",
        "    # Regression: Next Return\n",
        "    df['Target_Return'] = df['Close'].pct_change().shift(-1)\n",
        "    \n",
        "    # Regression: Next Close\n",
        "    df['Target_Close'] = df['Close'].shift(-1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"‚úÖ H√†m Feature Engineering ƒë√£ s·∫µn s√†ng!\")"
      ],
      "metadata": {
        "id": "fe_main_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. üöÄ Th·ª±c Hi·ªán Feature Engineering"
      ],
      "metadata": {
        "id": "run_fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"üöÄ B·∫ÆT ƒê·∫¶U FEATURE ENGINEERING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "timeframes = ['daily', '4h', '1h']\n",
        "\n",
        "processed_data = {}\n",
        "\n",
        "for name in CURRENCY_PAIRS.values():\n",
        "    processed_data[name] = {}\n",
        "    \n",
        "    for tf in timeframes:\n",
        "        input_path = f'data/raw/{tf}/{name}_{tf}.csv'\n",
        "        output_path = f'data/processed/{tf}/{name}_{tf}_processed.csv'\n",
        "        \n",
        "        if os.path.exists(input_path):\n",
        "            print(f\"\\nüìä ƒêang x·ª≠ l√Ω {name} ({tf})...\")\n",
        "            \n",
        "            # ƒê·ªçc d·ªØ li·ªáu\n",
        "            df = pd.read_csv(input_path)\n",
        "            original_cols = len(df.columns)\n",
        "            original_rows = len(df)\n",
        "            \n",
        "            # Feature Engineering\n",
        "            df_processed = add_all_features(df)\n",
        "            \n",
        "            # Lo·∫°i b·ªè NaN\n",
        "            df_clean = df_processed.dropna()\n",
        "            \n",
        "            # L∆∞u file\n",
        "            df_clean.to_csv(output_path, index=False)\n",
        "            \n",
        "            processed_data[name][tf] = df_clean\n",
        "            \n",
        "            print(f\"  ‚úÖ {original_cols} ‚Üí {len(df_clean.columns)} columns\")\n",
        "            print(f\"  ‚úÖ {original_rows} ‚Üí {len(df_clean)} rows\")\n",
        "            print(f\"  üíæ ƒê√£ l∆∞u: {output_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ FEATURE ENGINEERING HO√ÄN TH√ÄNH!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "run_fe_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# PH·∫¶N 3: KI·ªÇM TRA K·∫æT QU·∫¢\n",
        "---"
      ],
      "metadata": {
        "id": "part3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Xem M·∫´u D·ªØ Li·ªáu"
      ],
      "metadata": {
        "id": "check_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê·ªçc m·ªôt file m·∫´u\n",
        "sample_df = pd.read_csv('data/processed/daily/EUR_USD_daily_processed.csv')\n",
        "\n",
        "print(f\"üìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu: {sample_df.shape}\")\n",
        "print(f\"üìä S·ªë features: {len(sample_df.columns)}\")\n",
        "print(\"\\nüìã Danh s√°ch c√°c c·ªôt:\")\n",
        "print(sample_df.columns.tolist())"
      ],
      "metadata": {
        "id": "check_data_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xem 5 d√≤ng ƒë·∫ßu\n",
        "sample_df.head()"
      ],
      "metadata": {
        "id": "view_head"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Th·ªëng k√™ m√¥ t·∫£\n",
        "sample_df.describe()"
      ],
      "metadata": {
        "id": "describe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Ph√¢n Ph·ªëi Target Variable"
      ],
      "metadata": {
        "id": "target_dist"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution c·ªßa Target_Direction\n",
        "print(\"Ph√¢n ph·ªëi Target_Direction:\")\n",
        "print(sample_df['Target_Direction'].value_counts())\n",
        "print(f\"\\nT·ª∑ l·ªá Up/Down: {sample_df['Target_Direction'].mean()*100:.2f}% Up\")"
      ],
      "metadata": {
        "id": "target_dist_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. Li·ªát K√™ T·∫•t C·∫£ Files"
      ],
      "metadata": {
        "id": "list_files"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"üìÅ C·∫§U TR√öC TH∆Ø M·ª§C D·ªÆ LI·ªÜU:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for root, dirs, files in os.walk('data'):\n",
        "    level = root.replace('data', '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f'{indent}üìÇ {os.path.basename(root)}/')\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            filepath = os.path.join(root, file)\n",
        "            size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "            print(f'{subindent}üìÑ {file} ({size_mb:.2f} MB)')"
      ],
      "metadata": {
        "id": "list_files_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# PH·∫¶N 4: CHU·∫®N B·ªä CHO ML\n",
        "---"
      ],
      "metadata": {
        "id": "part4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. H√†m Load Data cho ML"
      ],
      "metadata": {
        "id": "ml_prep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_for_ml(pair='EUR_USD', timeframe='daily'):\n",
        "    \"\"\"\n",
        "    Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho ML\n",
        "    \n",
        "    Parameters:\n",
        "    - pair: 'EUR_USD', 'GBP_USD', 'USD_JPY', 'AUD_USD', 'USD_CHF'\n",
        "    - timeframe: 'daily', '4h', '1h'\n",
        "    \n",
        "    Returns:\n",
        "    - X: Features\n",
        "    - y_class: Target cho Classification\n",
        "    - y_reg: Target cho Regression\n",
        "    \"\"\"\n",
        "    filepath = f'data/processed/{timeframe}/{pair}_{timeframe}_processed.csv'\n",
        "    df = pd.read_csv(filepath)\n",
        "    \n",
        "    # Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng ph·∫£i features\n",
        "    exclude_cols = ['Date', 'Target_Direction', 'Target_Return', 'Target_Close']\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "    \n",
        "    X = df[feature_cols]\n",
        "    y_class = df['Target_Direction']\n",
        "    y_reg = df['Target_Return']\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {pair} ({timeframe})\")\n",
        "    print(f\"   Features: {X.shape[1]}\")\n",
        "    print(f\"   Samples: {X.shape[0]}\")\n",
        "    \n",
        "    return X, y_class, y_reg\n",
        "\n",
        "print(\"‚úÖ H√†m load_data_for_ml ƒë√£ s·∫µn s√†ng!\")"
      ],
      "metadata": {
        "id": "ml_prep_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V√≠ d·ª• load data\n",
        "X, y_class, y_reg = load_data_for_ml('EUR_USD', 'daily')"
      ],
      "metadata": {
        "id": "load_example"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16. Download Files v·ªÅ M√°y (T√πy ch·ªçn)"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N√©n v√† download t·∫•t c·∫£ data\n",
        "!zip -r forex_data.zip data/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('forex_data.zip')\n",
        "\n",
        "print(\"‚úÖ ƒê√£ t·∫°o file forex_data.zip ƒë·ªÉ download!\")"
      ],
      "metadata": {
        "id": "download_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# ‚úÖ HO√ÄN TH√ÄNH!\n",
        "\n",
        "Dataset ƒë√£ s·∫µn s√†ng ƒë·ªÉ train c√°c models ML:\n",
        "- **Classification**: Random Forest, XGBoost, LSTM\n",
        "- **Regression**: Linear Regression, Gradient Boosting\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "done"
      }
    }
  ]
}
